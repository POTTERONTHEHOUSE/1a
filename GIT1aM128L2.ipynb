{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIT1aM128L2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POTTERONTHEHOUSE/1a/blob/master/GIT1aM128L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bimF6xJPog43",
        "colab_type": "code",
        "outputId": "047bb160-c286-4c55-c91f-2d33c88a40c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwXK8I3dokua",
        "colab_type": "code",
        "outputId": "ab88ad20-c5e9-4f3c-b6d2-1d9d616fd866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install Cython #this does solve the \"NameError: name '_C' is not defined\" problem\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "# Hyper Parameters\n",
        "EPOCH =120         # never mind i got early stop\n",
        "\n",
        "BATCH_SIZE =128    \n",
        "LR = 0.001              # learning rate \n",
        "M=128\n",
        "L=2\n",
        "result=torch.tensor([0,0,0,0])# the output of the this program：[M,L,training accuracy, test accuaracy]\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=True, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:#so if score < self.best_score 7 times in a row, then self.early_stop = True.\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when test loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'test error decreased ({self.val_loss_min:.2f} --> {val_loss:.2f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "#torch.manual_seed(1)    # reproducible\n",
        "\n",
        "# prepare data for CNN\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,                                    \n",
        "    transform=transform,   \n",
        "\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Data Loader for easy mini-batch return in training\n",
        "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE , shuffle=True)\n",
        "\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False,download=True,transform=transform)\n",
        "testloader = Data.DataLoader(test_data, batch_size=BATCH_SIZE , shuffle=False)\n",
        "\n",
        "\n",
        "#for the first conv layer,as is discribed in the paper\n",
        "def _pool_function_factory(relu,conv):\n",
        "    def pool_function(inputs):\n",
        "      _output=pool(relu(conv(inputs)))\n",
        "      return _output\n",
        "    return pool_function\n",
        "#define a basic 'layer' with relu, without batch normalization\n",
        "def _bn_function_factory(conv,relu):\n",
        "  def bn_function(inputs):\n",
        "    _output=relu(conv(inputs))\n",
        "    return _output\n",
        "  return bn_function\n",
        "\n",
        "#define a recursive layer which includes norm and relu\n",
        "class _recursive(nn.Module):\n",
        "  def __init__(self,M):\n",
        "    super(_recursive,self).__init__()\n",
        "    self.add_module('convr',nn.Conv2d(M,M,kernel_size=3,padding=1,bias=True))\n",
        "    #self.add_module('normr',nn.BatchNorm2d(M))\n",
        "    self.add_module('relur',nn.ReLU())\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    #output=_bn_function_factory(self.convr,self.relur)\n",
        "    output=self.convr(inputs)\n",
        "    output=self.relur(output)\n",
        "    return output\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        #self.features1 = nn.Sequential(\n",
        "            #1\n",
        "            #nn.Conv2d(3,M,kernel_size=8,padding=4,bias=True),\n",
        "            #nn.ReLU(),\n",
        "            #nn.MaxPool2d(kernel_size=4,stride=4),\n",
        "        #)\n",
        "        #first layer\n",
        "        self.features=nn.Sequential(OrderedDict([\n",
        "                ('conv1', nn.Conv2d(3, M, kernel_size=8, padding=4, bias=True)),\n",
        "            ]))\n",
        "        '''self.conv1=nn.Conv2d(3, M, kernel_size=8, padding=4, bias=True),    \n",
        "        self.features=self.conv1,#then features become tuple and can not be added modules\n",
        "        #and it says 'tuple' object has no attribute 'size', strange why'''\n",
        "        \n",
        "        self.features.add_module('relu1',nn.ReLU()),\n",
        "        \n",
        "        self.features.add_module('pool1',nn.MaxPool2d(kernel_size=4,stride=4)),\n",
        "        \n",
        "        #2~L+1 layers,recursive\n",
        "        for i in range(1,L+1):\n",
        "          layer=_recursive(M)\n",
        "          self.features.add_module('recursivelayer%d'%(i),layer)\n",
        "        \n",
        "        #After the last hidden layer\n",
        "        self.classifier =  nn.Linear(M*8*8,num_classes)\n",
        "          \n",
        "        # Initialization\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'conv' in name and 'weight' in name:\n",
        "                param.data.normal_(0,0.1) \n",
        "            elif 'norm' in name and 'weight' in name:\n",
        "                param.data.fill_(1)\n",
        "            elif 'norm' in name and 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "            elif 'classifier' in name and 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out= self.features(x)\n",
        "        #print('out1=',out)\n",
        "        #print('out1size=',out.size())\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)#logistic classifier ;softmax is in nn.CrossEntropyLoss\n",
        "        return out    \n",
        "\n",
        "cnn = CNN()\n",
        "print(cnn)\n",
        "#initialization function\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__ # 得到网络层的名字，如ConvTranspose2d\n",
        "    if classname.find('Conv') != -1:  # 使用了find函数，如果不存在返回值为-1，所以让其不等于-1\n",
        "        m.weight.data.normal_(0.0, 0.1)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)#the paper did not mention the initialization of BN but whatever\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "cnn.apply(weight_init)\n",
        "\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)   \n",
        "# The L2 regularization on the parameters of the model is already included in most optimizers, including optim.SGD\n",
        "loss_func = nn.CrossEntropyLoss()                       \n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping()\n",
        "\n",
        "\n",
        "# training and testing\n",
        "for epoch in range(EPOCH):\n",
        "  run_correct=0 \n",
        "  train_accuracy=0\n",
        "  running_loss = 0.0\n",
        "    \n",
        "  for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
        "            \n",
        "      b_x=Variable(b_x,requires_grad=True)\n",
        "\n",
        "      output = cnn(b_x)            # cnn output\n",
        "      _,predictrain=torch.max(output,1)\n",
        "      num_correct=(predictrain==b_y).sum()\n",
        "      run_correct+=num_correct.item()\n",
        "\n",
        "      loss = loss_func(output, b_y)   # cross entropy loss\n",
        "      optimizer.zero_grad()           # clear gradients for this training step\n",
        "      loss.backward()                 # backpropagation, compute gradients\n",
        "      optimizer.step()                # apply gradients\n",
        "      # print statistics\n",
        "  train_accuracy=100*run_correct / (len(train_data))\n",
        "  print('Finish {0} epoch, training accuracy: {1:.2f}%--L={2},M={3}'.format(epoch + 1, train_accuracy,L,M))\n",
        "      \n",
        "      \n",
        "\n",
        "  total_correct = 0\n",
        "  total_images = 0\n",
        "  test_losses=[]\n",
        "  test_loss=0\n",
        "  avg_test_losses=[]\n",
        "  with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = cnn(images)\n",
        "            outloss=loss_func(outputs,labels)\n",
        "            test_losses.append(outloss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "        test_loss = np.average(test_losses)  \n",
        "        avg_test_losses.append(test_loss)  \n",
        "        test_losses = []\n",
        "\n",
        "  test_accuracy = total_correct / total_images * 100\n",
        "  print('In Epoch {0}, testing accuracy: {1:.2f}% --L={2},M={3}'.format(epoch+1,test_accuracy,L,M))\n",
        "\n",
        "  # load the last checkpoint with the best model for the early stop\n",
        "  early_stopping(test_loss, cnn)#has save model inside\n",
        "  if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "  # load the last checkpoint with the best model for the early stop\n",
        "\n",
        "result=[M,L,train_accuracy,test_accuracy]\n",
        "torch.save(result,F\"/content/gdrive/My Drive/{'resultM128L2.pt'}\" )\n",
        "print(result)\n",
        "   \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.13)\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "CNN(\n",
            "  (features): Sequential(\n",
            "    (conv1): Conv2d(3, 128, kernel_size=(8, 8), stride=(1, 1), padding=(4, 4))\n",
            "    (relu1): ReLU()\n",
            "    (pool1): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "    (recursivelayer1): _recursive(\n",
            "      (convr): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relur): ReLU()\n",
            "    )\n",
            "    (recursivelayer2): _recursive(\n",
            "      (convr): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relur): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=8192, out_features=10, bias=True)\n",
            ")\n",
            "Finish 1 epoch, training accuracy: 46.09%--L=2,M=128\n",
            "In Epoch 1, testing accuracy: 53.58% --L=2,M=128\n",
            "test error decreased (inf --> 1.31).  Saving model ...\n",
            "Finish 2 epoch, training accuracy: 58.71%--L=2,M=128\n",
            "In Epoch 2, testing accuracy: 58.12% --L=2,M=128\n",
            "test error decreased (1.31 --> 1.18).  Saving model ...\n",
            "Finish 3 epoch, training accuracy: 63.89%--L=2,M=128\n",
            "In Epoch 3, testing accuracy: 61.58% --L=2,M=128\n",
            "test error decreased (1.18 --> 1.08).  Saving model ...\n",
            "Finish 4 epoch, training accuracy: 67.14%--L=2,M=128\n",
            "In Epoch 4, testing accuracy: 64.03% --L=2,M=128\n",
            "test error decreased (1.08 --> 1.04).  Saving model ...\n",
            "Finish 5 epoch, training accuracy: 69.54%--L=2,M=128\n",
            "In Epoch 5, testing accuracy: 63.53% --L=2,M=128\n",
            "test error decreased (1.04 --> 1.03).  Saving model ...\n",
            "Finish 6 epoch, training accuracy: 71.59%--L=2,M=128\n",
            "In Epoch 6, testing accuracy: 63.30% --L=2,M=128\n",
            "EarlyStopping counter: 1 out of 7\n",
            "Finish 7 epoch, training accuracy: 73.61%--L=2,M=128\n",
            "In Epoch 7, testing accuracy: 65.64% --L=2,M=128\n",
            "test error decreased (1.03 --> 0.99).  Saving model ...\n",
            "Finish 8 epoch, training accuracy: 75.01%--L=2,M=128\n",
            "In Epoch 8, testing accuracy: 65.98% --L=2,M=128\n",
            "EarlyStopping counter: 1 out of 7\n",
            "Finish 9 epoch, training accuracy: 76.17%--L=2,M=128\n",
            "In Epoch 9, testing accuracy: 65.18% --L=2,M=128\n",
            "EarlyStopping counter: 2 out of 7\n",
            "Finish 10 epoch, training accuracy: 77.51%--L=2,M=128\n",
            "In Epoch 10, testing accuracy: 66.18% --L=2,M=128\n",
            "test error decreased (0.99 --> 0.99).  Saving model ...\n",
            "Finish 11 epoch, training accuracy: 78.76%--L=2,M=128\n",
            "In Epoch 11, testing accuracy: 67.05% --L=2,M=128\n",
            "test error decreased (0.99 --> 0.98).  Saving model ...\n",
            "Finish 12 epoch, training accuracy: 80.06%--L=2,M=128\n",
            "In Epoch 12, testing accuracy: 67.45% --L=2,M=128\n",
            "test error decreased (0.98 --> 0.96).  Saving model ...\n",
            "Finish 13 epoch, training accuracy: 80.95%--L=2,M=128\n",
            "In Epoch 13, testing accuracy: 67.14% --L=2,M=128\n",
            "EarlyStopping counter: 1 out of 7\n",
            "Finish 14 epoch, training accuracy: 81.96%--L=2,M=128\n",
            "In Epoch 14, testing accuracy: 67.42% --L=2,M=128\n",
            "EarlyStopping counter: 2 out of 7\n",
            "Finish 15 epoch, training accuracy: 83.06%--L=2,M=128\n",
            "In Epoch 15, testing accuracy: 67.81% --L=2,M=128\n",
            "EarlyStopping counter: 3 out of 7\n",
            "Finish 16 epoch, training accuracy: 83.63%--L=2,M=128\n",
            "In Epoch 16, testing accuracy: 66.42% --L=2,M=128\n",
            "EarlyStopping counter: 4 out of 7\n",
            "Finish 17 epoch, training accuracy: 84.64%--L=2,M=128\n",
            "In Epoch 17, testing accuracy: 67.72% --L=2,M=128\n",
            "EarlyStopping counter: 5 out of 7\n",
            "Finish 18 epoch, training accuracy: 85.79%--L=2,M=128\n",
            "In Epoch 18, testing accuracy: 68.24% --L=2,M=128\n",
            "EarlyStopping counter: 6 out of 7\n",
            "Finish 19 epoch, training accuracy: 86.74%--L=2,M=128\n",
            "In Epoch 19, testing accuracy: 67.38% --L=2,M=128\n",
            "EarlyStopping counter: 7 out of 7\n",
            "Early stopping\n",
            "[128, 2, 86.742, 67.38]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}